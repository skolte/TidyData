#### Author : Sandeep Kolte
#### Date : 3/19/2015

# README.md
##Contents
* ReadMe.md
* CodeBook.txt
* run_analysis.R

##README.md
This file.

##CodeBook.txt
CodeBook.txt is the code book for this class project. The purpose of the project is to demonstrate  ability to collect, work with, and clean a data set. The goal is to prepare tidy data that can be used for later analysis. 
This code book explains the tidy dataset generated by run_analysis.R script. It contains the following information:
Information about the variables, including clarification of units, in the dataset not contained in the tidy data.
Information about the summary choices made.
Information about the experimental design.

##run_analysis.R
###Instructions to run 'run_analysis.R'
* Download the original dataset zip file from https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip.
* Unzip it to a folder. 
* Place 'run_analysis.R' in same folder as the unzipped 'UCI HAR DATASET' folder.
* Run 'run_analysis.R' from R command line as 'run_analysis()'. It has been tested with R version 3.1.2 (2014-10-31).


##Technique Used to build Tiny Dataset

###Original Data
The original dataset, Human Activity Recognition Using Smartphones Dataset Version 1.0, was compiled by a group of students working in "Smartlab - Non Linear Complex Systems Laboratory".
The data linked to from the course website represents data collected from the accelerometers from the Samsung Galaxy S smartphone. A full description is available at the site where the data was obtained: 
http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones 

The original dataset for this project is available at :
https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip 

### Requirements
Create one R script called run_analysis.R that does the following. 
* Merges the training and the test sets to create one data set.
* Extracts only the measurements on the mean and standard deviation for each measurement. 
* Uses descriptive activity names to name the activities in the data set
* Appropriately labels the data set with descriptive variable names. 
* From the data set in step 4, creates a second, independent tidy data set with the average of each variable for each activity and each subject.

### Packages
The R script run_analysis.R requires three packages :
* data.table
* plyr
* dplyr

If the packages are not installed, the script will attempt to automatically install them.
       
### Explaining 'run_analysis.R'
Folder with the original dataset to be analyzed, /UCI HAR DATASET/, needs to be in same place as the R script. Code checks the current directory where 
the R script is located and if 'UCI HAR DATASET' directory is not found in the same location, it will raise an error informing the user.
        
Upon successfully finding the original dataset folder 'UCI HAR DATASET' the script begins by reading 'features.txt'. This is the list of all features. There are 561 features. These become column headers for the X_train and X_test data.
        
The script temporarily stores the column names in the 'features' data table. Next, it adds two new columns to support additional features - 'Activity_ID' and 'Subject'. Finally, it gets a list of all 563 features by combining 561 features read from the 'features.txt' and the two new additional additional features.
        
In order to implement the second requirement of the project, the script gets a list of features where feature name contains 'mean()' or 'std()', including the two new features, 'Activity_ID' and 'Subject'. This returns 33 with 'mean()' and 33 with 'std()'. With two new additional features 'Activity_ID' and 'Subject', finally it gets a list of 68 features. Also, it is important to note that when choosing the features with 'mean()' and 'std()' this script chooses to exclude any columns that have 'meanFreq' in the feature name in order to be consistent and to have a symmetrical division of 'Mean' and 'Standard Deviation' columns. There are no corresponding 'stdFreq' columns in the dataset for 'Standard Deviation', their absence may leave the reader/user confused searching for missing columns.

Next, the script reads the subjects measurements from two files 'train/subject_train.txt' and 'test/subject_test.txt'. Each row identifies the subject who performed the activity for each window sample. Its range is from 1 to 30. The analysis of these files reveals 2947 test observations and 7352 training observations. The combined dataset comprises of 10299 subject observations.
       
Next, the script reads 'activity_labels.txt'. This links the class labels with their activity name. 'Activity' signifies each activity a person was performing when measurements were taken. Each individual participating in the experiment performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING). This file has 6 activities each identified by a unique ID. 

In order to satisfy the requirement to replace the numeric values of the activities by the actual activity names the script sets column names of the activities data table such that the activity identifier column is set to 'Activity_ID' and the activity column name is set to 'Activity'. Also the script sets 'Activity_ID' as a key so that it can be used when replacing the numeric values with the actual activity name. 
        
Next, the script reads the "train/y_train.txt" and "test/y_test.txt" files. This is the activities data. The range of values is from 1 to 6. The analysis of these files reveals 2947 test observations and 7352 training observations. The combined dataset comprises of 10299 activity observations.
        
Next, the script reads the 'train/X_train.txt' and "test/X_test.txt". These are the values depicting sensor measurements. The analysis of these files reveals 2947 test observations and 7352 training observations. The combined dataset comprises of 10299 measurement observations in 561 variables each. The range of values is [-1, 1]. This is in accordance with the ReadMe file that came with original dataset which notes:

``` Features are normalized and bounded within [-1,1]. ```
       
At this point, the script has read all the necessary files and has built tables holding the data, it can now process the data. It begins creating a tidy dataset by adding column headers to the table that holds the measurements of all 561 features, using the features table built earlier. Also it assigns the data for the two additional new feature columns, 'Activity_ID' and 'Subject'. At this point, the scripts builds a dataset that has the combined data from training and test, all the columns have headers and dataset contains two new additional columns 'Activity_ID' and 'Subject' with data in them.
       
In order to satisfy requirement #2 of this project, the script selects only the columns that are measurements on the mean and standard deviation by filtering out all the other columns. This leaves a total of 68 columns, with 33 for 'mean()', 33 for 'std()' and two for 'Activity_ID' and 'Subject'. The script also sets 'Activity_ID' as a key in this new table. Next, it merges the filtered data with activities data to get a new column with the actual activity name for each activity in the dataset. At this point the dataset has 69 columns, with 'Activity', 'Activity_ID' and 'Subject' columns in addition to the 66 columns for 'mean()' and 'std()'. Since 'Activity_ID' is no longer required, the script removes it.and groups the results by 'Subject' and 'Activity'.
        
Next, the script calls 'getReadableColumnNames()' method to appropriately label the data set with descriptive variable names. A method simply returns a vector with the new modified descriptive variable names. This code is refactored to keep the main function concise. The description for each variable name is included in the Code Book. When naming the variables a unique, unambiguous name has been given to each variable. Variables names consist of one string ONLY, consisting of letters and dots (.). The form for variable names is all lower case letters and words separated with dots (variable.name). Spaces have not been used in variables names similar to most other statistical programs, even if data entry programs like Excel or Access will allow this. Variables names have been entered at the top of each column. Variables names are chosen to be long enough to be meaningful, but short enough to be easy to handle. 
It is important to note that "Google's R Style Guide" is referenced for guidelines when naming the variables. For more info : https://google-styleguide.googlecode.com/svn/trunk/Rguide.xml#identifiers

Finally, the script creates an independent tidy data set with the average of each variable for each activity and each subject. The final tidy dataset contains 180 observation rows with a total of 68 columns.
        
